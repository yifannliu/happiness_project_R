data = read.csv('https://raw.githubusercontent.com/arunagossai/happiness_project_R/master/happiness_data.csv', header = TRUE)
head(data)
summary(data)
dim(data)
#install.packages('fastDummies')
#install.packages('qdapTools')
#install.packages('glmnet')
#install.packages('tseries')
#install.packages("tidyverse")
#install.packages("broom")
library(tseries)
library(fastDummies)
library(qdapTools)
library(glmnet)
library(tidyverse)
library(broom)
#Creating a key for the regions, will use later
regionkey = levels(data$region)
regionvalues = c(1:10)
key = data.frame(regionkey,regionvalues)
#Changing the region from categorical to numeric
data$region <- as.numeric(data$region)
#Change getting rid of countries with no observations, and countries with no democracy value
df1  = subset(data, country != "Kosovo"  & country !="Taiwan" & country!="Sudan" & democracy!= 'NA')
paste( dim(data)[1] - dim(df1)[1], "observations lost")
#Taking the mean of each column by country. Changes dataset from pooled cross-sectional to cross-sectional 
df2 <- aggregate.data.frame(df1[-2], by = list(df1$country), mean)
paste( dim(df1)[1] - dim(df2)[1], "observations lost")
#adding a column for the region name
rname = lookup(df2$region,key$regionvalues,key$regionkey)
df = data.frame(df2,rname)
#Creating dummy variables from the region name
df_dum <- dummy_cols(df, select_columns = "rname")
#testing for multicollinearity excluding regions and year from matrix
cor(df[6:15])
#serious issues with multicollinearity, dropping the problem variables
df$men_edu <- NULL
df$sanitation <- NULL
df$elder_child <- NULL
df$child_mortality <- NULL
df_dum$men_edu <- NULL
df_dum$sanitation <- NULL
df_dum$elder_child <- NULL
df_dum$child_mortality <- NULL
#dropping variables that are not needed
df$year <- NULL
df$ï..id <- NULL
df$region <- NULL
df_dum$year <- NULL
df_dum$ï..id <- NULL
df_dum$region <- NULL
df_dum$rname <- NULL  
df_dum$rname_West_EU <- NULL #getting rid of one dummy variable to prevent multicollinearity
#MODEL 3: LINEAR MODEL WITH ALL VARIABLES
set.seed(123)
n = nrow(df_dum)
Index = sample(1:n, size = round(0.7*n), replace=FALSE)
train = df_dum[Index,]
test = df_dum[-Index,]  
head(df_dum)
# linera model - full
M3.1 = lm(happiness ~ democracy + gini + refugee +  women_edu + pop_den + labour, train[2:17])
pred_base = predict(M3.1, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.1, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# linera model with all varialbes except labour
M3.2 = lm(happiness ~ democracy + gini + refugee +  women_edu + pop_den, train[2:17])
pred_base = predict(M3.2, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.2, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# no pop_den
M3.3 = lm(happiness ~ democracy + gini + refugee + women_edu + labour, train[2:17])
pred_base = predict(M3.3, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.3, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# no women_edu
M3.4 = lm(happiness ~ democracy + gini + refugee + pop_den + labour, train[2:17])
pred_base = predict(M3.4, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.4, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# no refugee
M3.5 = lm(happiness ~ democracy + gini + women_edu + pop_den + labour, train[2:17])
pred_base = predict(M3.5, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.5, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# no gini
M3.6 = lm(happiness ~ democracy + refugee + women_edu + pop_den + labour, train[2:17])
pred_base = predict(M3.6, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.6, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# no democracy
M3.7 = lm(happiness ~ gini + refugee + women_edu + pop_den + labour, train[2:17])
pred_base = predict(M3.7, test[2:17])
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
pred_base_in = predict(M3.7, train[2:17])
RMSE_BASE_in = sqrt(sum((pred_base_in-train$happiness)^2)/length(pred_base_in))
RMSE_BASE_in
# out-sample: 
# M3.5 [0.7118189] 
# M3.6 [0.7321898] 
# M3.1 [0.7328362] 
# M3.3 [0.7336832] 
# M3.2 [0.734806] 
# M3.7[0.769251]
# M3.4 [0.8~] 
# in-sample: 
# M3.1 [0.6906424] 
# M3.2 [0.6906705] 
# M3.6 [0.690838] 
# M3.3 [0.6919279] 
# M3.5 [0.69511158] 
# M3.7 [0.7~]
# M3.4 [0.8~] 
# model 3.6 is selected for a balance of low in-sample and out-sample error 
# QQ plot for residual diagnostic
par(mfrow=c(2,2))
plot(M3.6)
# overall good residual features
# get regression results and adjusted R-squared
summar(M3.6)
# get CV
cbind(M3.6$coefficients,confint(M3.6))
